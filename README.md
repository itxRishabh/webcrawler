<p align="center">
  <h1 align="center">ğŸŒ Web Crawler</h1>
  <p align="center">
    A modern, production-ready web application for creating offline archives of websites.
    <br />
    Built with <strong>Next.js</strong> and <strong>Node.js</strong>
    <br />
    <br />
    <a href="#-quick-start">Quick Start</a>
    Â·
    <a href="#-api-reference">API Docs</a>
    Â·
    <a href="#-features">Features</a>
    Â·
    <a href="#%EF%B8%8F-legal-disclaimer">Legal</a>
  </p>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License" />
  <img src="https://img.shields.io/badge/node-%3E%3D20.0.0-green.svg" alt="Node" />
  <img src="https://img.shields.io/badge/typescript-5.x-blue.svg" alt="TypeScript" />
  <img src="https://img.shields.io/badge/next.js-14.x-black.svg" alt="Next.js" />
  <img src="https://img.shields.io/badge/docker-ready-2496ED.svg" alt="Docker" />
</p>

---

## ğŸ“‹ Table of Contents

- [âš ï¸ Legal Disclaimer](#%EF%B8%8F-legal-disclaimer)
- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ—ï¸ Architecture](#%EF%B8%8F-architecture)
- [ğŸ“¡ API Reference](#-api-reference)
- [âš™ï¸ Configuration](#%EF%B8%8F-configuration)
- [ğŸš« Limitations](#-limitations)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ“„ License & Legal](#-license--legal)
- [ğŸ¤ Contributing](#-contributing)

---

## âš ï¸ Legal Disclaimer

> **â›” THE DEVELOPERS OF THIS SOFTWARE ARE NOT RESPONSIBLE FOR ANY MISUSE OF THIS TOOL.**

This tool is provided **strictly for educational and legitimate purposes** such as:

| âœ… Permitted Uses | âŒ Prohibited Uses |
|-------------------|-------------------|
| Creating offline backups of **your own websites** | Copying websites you don't own |
| Archiving publicly available content for personal use | Violating Terms of Service |
| Web development and testing purposes | Copyright/trademark infringement |
| Academic research and study | Circumventing access controls |
| | Redistributing copied content |

### ğŸ”’ User Responsibility

By using this software, you acknowledge:

1. **You are solely responsible** for ensuring compliance with all applicable laws
2. **Copying websites without permission may constitute copyright infringement**
3. **The developers disclaim all liability** for any damages or legal consequences
4. **You will respect `robots.txt`** and rate limits

ğŸ“– **Read the full disclaimer:** [DISCLAIMER.md](DISCLAIMER.md)

---

## âœ¨ Features

### Core Functionality

| Feature | Description |
|---------|-------------|
| ğŸŒ **Complete Website Mirroring** | Download entire websites with all assets for offline browsing |
| ğŸ”— **Intelligent Link Rewriting** | All internal links automatically rewritten to work locally |
| ğŸ“¦ **Asset Preservation** | Preserves HTML, CSS, JavaScript, images, fonts, and media files |
| âš¡ **Real-time Progress** | WebSocket-based live updates, logging, and progress tracking |
| ğŸ“¥ **ZIP Download** | Download complete archives ready for offline use |

### Security Features

| Feature | Description |
|---------|-------------|
| ğŸ›¡ï¸ **SSRF Protection** | Blocks internal IPs, localhost, and cloud metadata endpoints |
| ğŸ”’ **DNS Rebinding Prevention** | Validates resolved IPs before fetching |
| â±ï¸ **Rate Limiting** | Per-user request throttling to prevent abuse |
| âœ… **Input Validation** | All inputs validated using Zod schemas |
| ğŸ“ **Path Traversal Prevention** | Sanitized filenames and directory paths |
| ğŸ“Š **Configurable Limits** | Max file size, total size, depth, and page limits |

### Technical Highlights

- ğŸ³ **Docker Ready** - Production deployment with Docker Compose
- ğŸ”„ **BFS Crawling** - Breadth-first search with URL deduplication
- ğŸ¤– **robots.txt Support** - Optional compliance with robots.txt directives  
- ğŸ”„ **Retry Logic** - Automatic retry with exponential backoff
- ğŸ“ **Comprehensive Logging** - Detailed crawl logs for debugging

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** >= 20.0.0
- **npm** >= 9.0.0
- **Docker** (optional, for containerized deployment)

### Option 1: Using Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/itxRishabh/webcrawler.git
cd webcrawler

# Start with Docker Compose
docker-compose up --build

# Access the UI at http://localhost:3000
```

### Option 2: Manual Development Setup

```bash
# Clone the repository
git clone https://github.com/itxRishabh/webcrawler.git
cd webcrawler

# Setup Backend
cd backend
cp .env.example .env
npm install
npm run dev

# Setup Frontend (in a new terminal)
cd frontend
npm install
npm run dev

# Access the UI at http://localhost:3000
# Backend API at http://localhost:3001
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Frontend       â”‚  HTTP   â”‚       Backend       â”‚
â”‚     (Next.js)       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     (Node.js)       â”‚
â”‚    Port: 3000       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Port: 3001       â”‚
â”‚                     â”‚   WS    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â–¼                â–¼                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Queue   â”‚    â”‚ Fetcher  â”‚    â”‚ Storage  â”‚
                    â”‚  (BFS)   â”‚    â”‚ (HTTP)   â”‚    â”‚  (ZIP)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend Components

| Component | File | Description |
|-----------|------|-------------|
| **API Routes** | `src/api/routes.ts` | REST endpoints for job management |
| **WebSocket** | `src/api/websocket.ts` | Real-time progress updates |
| **Crawl Engine** | `src/crawler/engine.ts` | Main crawling orchestrator |
| **Queue** | `src/crawler/queue.ts` | BFS URL queue with deduplication |
| **Fetcher** | `src/crawler/fetcher.ts` | HTTP client with retry logic |
| **HTML Parser** | `src/crawler/parsers/html.ts` | Link extraction using Cheerio |
| **CSS Parser** | `src/crawler/parsers/css.ts` | URL extraction using PostCSS |
| **Rewriter** | `src/crawler/rewriter.ts` | URL to filesystem path conversion |
| **Storage** | `src/storage/filesystem.ts` | File writing with collision handling |
| **Archiver** | `src/storage/archiver.ts` | ZIP archive creation |
| **SSRF Protection** | `src/security/ssrf.ts` | IP validation and blocking |
| **Validation** | `src/security/validation.ts` | Input sanitization with Zod |

### Frontend Components

| Component | Description |
|-----------|-------------|
| **UrlInput** | URL entry form with validation |
| **AdvancedOptions** | Crawl configuration settings |
| **CrawlProgress** | Real-time progress display |
| **LogViewer** | Live crawl log streaming |
| **DownloadPanel** | Archive download interface |

---

## ğŸ“¡ API Reference

Base URL: `http://localhost:3001/api`

### Jobs

#### Create a New Job

```http
POST /api/jobs
Content-Type: application/json
```

**Request Body:**

```json
{
  "url": "https://example.com",
  "maxDepth": 5,
  "maxPages": 500,
  "scope": "same-domain",
  "respectRobotsTxt": true,
  "concurrency": 5,
  "delayMs": 100,
  "fileTypes": {
    "html": true,
    "css": true,
    "js": true,
    "images": true,
    "fonts": true,
    "media": true
  }
}
```

**Request Parameters:**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `url` | string | *required* | Target URL to crawl (http/https only) |
| `maxDepth` | number | 50 | Maximum crawl depth (1-1000) |
| `maxPages` | number | 10000 | Maximum pages to crawl (1-1000000) |
| `scope` | string | "same-domain" | Crawl scope: `same-domain`, `same-host`, `subdomains` |
| `respectRobotsTxt` | boolean | false | Honor robots.txt directives |
| `concurrency` | number | 5 | Concurrent requests (1-50) |
| `delayMs` | number | 100 | Delay between requests in ms (0-10000) |
| `timeoutMs` | number | 60000 | Request timeout in ms (1000-300000) |
| `userAgent` | string | - | Custom User-Agent header |
| `cookies` | string | - | Cookies to send with requests |
| `unlimitedMode` | boolean | false | Remove most crawl limits |

**Response:**

```json
{
  "success": true,
  "job": {
    "id": "uuid-string",
    "url": "https://example.com",
    "status": "pending",
    "progress": { "pagesProcessed": 0, "totalPages": 0 },
    "createdAt": "2026-01-21T10:00:00Z"
  }
}
```

---

#### List All Jobs

```http
GET /api/jobs
```

**Response:**

```json
{
  "success": true,
  "jobs": [
    {
      "id": "uuid-string",
      "url": "https://example.com",
      "status": "completed",
      "hasArchive": true
    }
  ]
}
```

---

#### Get Job Status

```http
GET /api/jobs/:id
```

**Response:**

```json
{
  "success": true,
  "job": {
    "id": "uuid-string",
    "url": "https://example.com",
    "status": "running",
    "progress": {
      "pagesProcessed": 50,
      "totalPages": 100,
      "bytesDownloaded": 5242880
    },
    "createdAt": "2026-01-21T10:00:00Z",
    "startedAt": "2026-01-21T10:00:05Z"
  }
}
```

**Job Status Values:**

| Status | Description |
|--------|-------------|
| `pending` | Job created, waiting to start |
| `running` | Actively crawling |
| `paused` | Temporarily paused |
| `completed` | Successfully finished |
| `failed` | Encountered an error |
| `cancelled` | Manually cancelled |

---

#### Start Job

```http
POST /api/jobs/:id/start
```

---

#### Pause Job

```http
POST /api/jobs/:id/pause
```

---

#### Resume Job

```http
POST /api/jobs/:id/resume
```

---

#### Cancel Job

```http
POST /api/jobs/:id/cancel
```

---

#### Delete Job

```http
DELETE /api/jobs/:id
```

---

#### Get Job Logs

```http
GET /api/jobs/:id/logs?since=0
```

**Query Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| `since` | number | Return logs after this timestamp |

---

#### Download Archive

```http
GET /api/jobs/:id/download
```

**Response:** ZIP file download

---

#### Preview File

```http
GET /api/jobs/:id/preview/:path
```

Example: `GET /api/jobs/abc123/preview/index.html`

---

### WebSocket Events

Connect to `ws://localhost:3001/ws`

**Subscribe to Job:**
```json
{ "type": "subscribe", "jobId": "uuid-string" }
```

**Event Types:**

| Event | Description |
|-------|-------------|
| `job:progress` | Progress update with current stats |
| `job:log` | New log entry |
| `job:complete` | Job finished successfully |
| `job:failed` | Job failed with error |

---

## âš™ï¸ Configuration

### Backend Environment Variables

Create a `.env` file in the `backend/` directory:

```env
# Server
PORT=3001
NODE_ENV=development

# Job Limits
MAX_CONCURRENT_JOBS=5
MAX_PAGES_PER_JOB=10000
MAX_FILE_SIZE_BYTES=52428800      # 50MB
MAX_TOTAL_SIZE_BYTES=1073741824   # 1GB
MAX_DEPTH=50

# Timeouts
DEFAULT_TIMEOUT_MS=60000

# Cleanup
CLEANUP_INTERVAL_MS=3600000       # 1 hour
JOB_TTL_MS=86400000               # 24 hours

# Rate Limiting
RATE_LIMIT_WINDOW_MS=60000        # 1 minute
RATE_LIMIT_MAX=10                 # 10 requests per window

# Crawler Defaults
DEFAULT_CONCURRENCY=5
DEFAULT_DELAY_MS=100
DEFAULT_USER_AGENT=WebCrawler/1.0

# Storage
JOBS_DIR=./jobs
TEMP_DIR=./temp

# Security
ALLOWED_PROTOCOLS=http,https
BLOCKED_HOSTS=
```

### Frontend Environment Variables

Create a `.env.local` file in the `frontend/` directory:

```env
NEXT_PUBLIC_API_URL=http://localhost:3001
NEXT_PUBLIC_WS_URL=ws://localhost:3001
```

---

## ğŸš« Limitations

| Feature | Status | Notes |
|---------|:------:|-------|
| Static HTML websites | âœ… | Full support |
| CSS stylesheets | âœ… | Including @import and url() |
| JavaScript files | âœ… | Downloaded as static files |
| Images & media | âœ… | All common formats |
| Fonts | âœ… | WOFF, WOFF2, TTF, etc. |
| JavaScript SPAs | âŒ | React/Vue/Angular require JS execution |
| Login-protected pages | âŒ | Credentials not supported |
| Dynamic/AJAX content | âŒ | Snapshot-based only |
| Infinite scroll | âŒ | Requires JS execution |
| WebSocket content | âŒ | Real-time data not captured |
| PDF/document generation | âŒ | Only existing files |

---

## ğŸ“ Project Structure

```
webcrawler/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/              # REST & WebSocket handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.ts     # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ websocket.ts  # Real-time updates
â”‚   â”‚   â”œâ”€â”€ config/           # Environment config
â”‚   â”‚   â”œâ”€â”€ crawler/          # Core crawling logic
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.ts     # Main orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ fetcher.ts    # HTTP client
â”‚   â”‚   â”‚   â”œâ”€â”€ queue.ts      # URL queue
â”‚   â”‚   â”‚   â”œâ”€â”€ rewriter.ts   # Link rewriting
â”‚   â”‚   â”‚   â”œâ”€â”€ robots.ts     # robots.txt parser
â”‚   â”‚   â”‚   â””â”€â”€ parsers/      # HTML/CSS parsers
â”‚   â”‚   â”œâ”€â”€ security/         # Security utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ ssrf.ts       # SSRF protection
â”‚   â”‚   â”‚   â””â”€â”€ validation.ts # Input validation
â”‚   â”‚   â”œâ”€â”€ storage/          # File management
â”‚   â”‚   â”‚   â”œâ”€â”€ archiver.ts   # ZIP creation
â”‚   â”‚   â”‚   â””â”€â”€ filesystem.ts # File operations
â”‚   â”‚   â””â”€â”€ utils/            # Helpers
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/              # Next.js App Router
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx      # Main page
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx    # Root layout
â”‚   â”‚   â”‚   â”œâ”€â”€ disclaimer/   # Legal pages
â”‚   â”‚   â”‚   â”œâ”€â”€ dmca/
â”‚   â”‚   â”‚   â”œâ”€â”€ privacy/
â”‚   â”‚   â”‚   â””â”€â”€ terms/
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ UrlInput.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AdvancedOptions.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ CrawlProgress.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LogViewer.tsx
â”‚   â”‚   â”‚   â””â”€â”€ DownloadPanel.tsx
â”‚   â”‚   â””â”€â”€ lib/              # Utilities
â”‚   â”‚       â””â”€â”€ api.ts        # API client
â”‚   â”œâ”€â”€ public/               # Static assets
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ DISCLAIMER.md
â”œâ”€â”€ SECURITY.md
â””â”€â”€ CODE_OF_CONDUCT.md
```

---

## ğŸ“„ License & Legal

| Document | Description |
|----------|-------------|
| [LICENSE](LICENSE) | MIT License with additional disclaimer |
| [DISCLAIMER.md](DISCLAIMER.md) | Comprehensive legal disclaimer |
| [SECURITY.md](SECURITY.md) | Security policy & built-in protections |
| [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) | Community guidelines |

**This software is provided "AS IS" without warranty of any kind. Use at your own risk.**

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork** the repository
2. **Clone** your fork locally
3. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
4. **Commit** your changes (`git commit -m 'Add amazing feature'`)
5. **Push** to the branch (`git push origin feature/amazing-feature`)
6. **Open** a Pull Request

### Development Guidelines

- Follow existing code style
- Add tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting

---

## ğŸ‘¨â€ğŸ’» Author

**Rishabh**

- GitHub: [@itxRishabh](https://github.com/itxRishabh)

---

<p align="center">
  <strong>âš ï¸ Always obtain proper authorization before crawling any website.</strong>
  <br />
  When in doubt, don't crawl.
  <br />
  <em>The developers are not liable for any misuse of this tool.</em>
</p>
